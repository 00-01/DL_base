{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "current.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "_hvSY3X-AeFP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import bottleneck\n",
    "from keras import backend as K\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "from PIL import Image\n",
    "from scipy.special import softmax\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 15:14:32.151224: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 15:14:32.479700: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-23 15:14:33.763849: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-23 15:14:33.763956: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-23 15:14:33.763963: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ],
   "metadata": {
    "id": "3wFty_P6dfBB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MODEL"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xmwhyyS7CFyK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class Bottleneck(keras.Model):\n",
    "  def __init__(self, expansion, stride, block_id, filters, alpha=1, ):\n",
    "    super(Bottleneck, self).__init__(name=\"Bottleneck_\"+block_id)\n",
    "    self.stride = stride\n",
    "    self.expansion = expansion\n",
    "    self.alpha = alpha\n",
    "    self.output_channels = self.alpha*filters\n",
    "    self.out = None  # there was some problem with the eager execution\n",
    "\n",
    "    prefix = 'Bottleneck_{}_'.format(block_id)\n",
    "    self.prefix = prefix\n",
    "    # expansion\n",
    "    self.expand_BN = layers.BatchNormalization(name=prefix+'expand_BN')\n",
    "    self.expand_ReLU = layers.ReLU(max_value=6, name=prefix+'expand_ReLU')\n",
    "\n",
    "    #conv\n",
    "    self.Conv = layers.DepthwiseConv2D(kernel_size=3, padding='same', strides=self.stride, use_bias=False,\n",
    "                                       name=prefix+'conv')\n",
    "    self.Conv_BN = layers.BatchNormalization(name=prefix+'conv_BN')\n",
    "    self.Conv_ReLU = layers.ReLU(max_value=6, name=prefix+'conv_ReLU')\n",
    "\n",
    "    #project\n",
    "    self.project = layers.Conv2D(filters=self.output_channels, kernel_size=1, use_bias=False, name='contract')\n",
    "    self.project_BN = layers.BatchNormalization(name=prefix+'contract_BN')\n",
    "\n",
    "    # dimensions need to be the same for residual connection\n",
    "    self.residual = layers.Add(name=prefix+'residual')\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.d = input_shape[-1]\n",
    "    self.expand = layers.Conv2D(filters=self.expansion*self.d, kernel_size=1, use_bias=False, name=self.prefix+'expand')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.expand(inputs)\n",
    "    x = self.expand_BN(x)\n",
    "    x = self.expand_ReLU(x)\n",
    "    self.out = x\n",
    "\n",
    "    x = self.Conv(x)\n",
    "    x = self.Conv_BN(x)\n",
    "    x = self.Conv_ReLU(x)\n",
    "\n",
    "    x = self.project(x)\n",
    "    x = self.project_BN(x)\n",
    "\n",
    "    if self.output_channels == self.d and self.stride == 1:\n",
    "      x = self.residual([inputs, x])\n",
    "\n",
    "    return x\n",
    "\n",
    "  def model(self):\n",
    "    x = keras.Input(shape=(28, 28, 3))\n",
    "\n",
    "    return keras.Model(inputs=[x], outputs=self.call(x))"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VYGagWE8T2Et",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#using the architecture mentioned in the paper\n",
    "class MobileNetv2(keras.Model):\n",
    "  def __init__(self, k=11):\n",
    "    super(MobileNetv2, self).__init__()\n",
    "    self.conv_inp = layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), padding='valid', use_bias=False, name='conv')\n",
    "    self.k = k\n",
    "\n",
    "    self.pad = layers.ZeroPadding2D(padding=2, name='pad')\n",
    "    self.BN = layers.BatchNormalization(name='BN')\n",
    "    self.ReLU = layers.ReLU(max_value=6, name='ReLU')\n",
    "\n",
    "    self.B1_1 = Bottleneck(expansion=1, filters=16, stride=1, block_id='B1_1')\n",
    "\n",
    "    self.B2_1 = Bottleneck(expansion=6, filters=24, stride=2, block_id='B2_1')\n",
    "    self.B2_2 = Bottleneck(expansion=6, filters=24, stride=1, block_id='B2_2')\n",
    "\n",
    "    self.B3_1 = Bottleneck(expansion=6, filters=32, stride=2, block_id='B3_1')\n",
    "    self.B3_2 = Bottleneck(expansion=6, filters=32, stride=1, block_id='B3_2')\n",
    "    self.B3_3 = Bottleneck(expansion=6, filters=32, stride=1, block_id='B3_3')\n",
    "\n",
    "    self.B4_1 = Bottleneck(expansion=6, filters=64, stride=2, block_id='B4_1')\n",
    "    self.B4_2 = Bottleneck(expansion=6, filters=64, stride=1, block_id='B4_2')\n",
    "    self.B4_3 = Bottleneck(expansion=6, filters=64, stride=1, block_id='B4_3')\n",
    "    self.B4_4 = Bottleneck(expansion=6, filters=64, stride=1, block_id='B4_4')\n",
    "\n",
    "    self.B5_1 = Bottleneck(expansion=6, filters=96, stride=1, block_id='B5_1')\n",
    "    self.B5_2 = Bottleneck(expansion=6, filters=96, stride=1, block_id='B5_2')\n",
    "    self.B5_3 = Bottleneck(expansion=6, filters=96, stride=1, block_id='B5_3')\n",
    "\n",
    "    self.B6_1 = Bottleneck(expansion=6, filters=160, stride=2, block_id='B6_1')\n",
    "    self.B6_2 = Bottleneck(expansion=6, filters=160, stride=1, block_id='B6_2')\n",
    "    self.B6_3 = Bottleneck(expansion=6, filters=160, stride=1, block_id='B6_3')\n",
    "\n",
    "    self.B7_1 = Bottleneck(expansion=6, filters=320, stride=1, block_id='B7_1')\n",
    "\n",
    "    self.conv_out = layers.Conv2D(filters=1280, kernel_size=1, strides=(1, 1), use_bias=False, name='conv_out')\n",
    "    self.avgpool = layers.AveragePooling2D(pool_size=(7, 7), name='avg_pool')\n",
    "\n",
    "    self.conv_seg = layers.Conv2D(filters=self.k, kernel_size=1, strides=(1, 1), use_bias=False, name='conv_seg')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.conv_inp(inputs)\n",
    "    x = self.BN(x)\n",
    "    x = self.ReLU(x)\n",
    "\n",
    "    x = self.B1_1(x)\n",
    "    x = self.B2_1(x)\n",
    "    x = self.B2_2(x)\n",
    "\n",
    "    x = self.B3_1(x)\n",
    "    x = self.B3_2(x)\n",
    "    x = self.B3_3(x)\n",
    "\n",
    "    x = self.B4_1(x)\n",
    "    x = self.B4_2(x)\n",
    "    x = self.B4_3(x)\n",
    "    x = self.B4_4(x)\n",
    "\n",
    "    x = self.B5_1(x)\n",
    "    x = self.B5_2(x)\n",
    "    x = self.B5_3(x)\n",
    "\n",
    "    x = self.B6_1(x)\n",
    "    x = self.B6_2(x)\n",
    "    x = self.B6_3(x)\n",
    "\n",
    "    x = self.B7_1(x)\n",
    "\n",
    "    x = self.conv_out(x)\n",
    "    x = self.avgpool(x)\n",
    "    c4 = self.conv_seg(x)\n",
    "\n",
    "    return c4\n",
    "\n",
    "  def model(self):\n",
    "    x = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "    return keras.Model(inputs=x, outputs=self.call(x))"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "05i7363EYwmP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a1d0ed91-6aff-4bdd-c04e-f2ca01ea3cad",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "MobileNetv2().model().summary()"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/z/.local/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:562: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 15:14:34.716348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 15:14:34.743521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 15:14:34.743789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv (Conv2D)               (None, 111, 111, 32)      864       \n",
      "                                                                 \n",
      " BN (BatchNormalization)     (None, 111, 111, 32)      128       \n",
      "                                                                 \n",
      " ReLU (ReLU)                 (None, 111, 111, 32)      0         \n",
      "                                                                 \n",
      " Bottleneck_B1_1 (Bottleneck  (None, 111, 111, 16)     2144      \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B2_1 (Bottleneck  (None, 56, 56, 24)       5568      \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B2_2 (Bottleneck  (None, 56, 56, 24)       9456      \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B3_1 (Bottleneck  (None, 28, 28, 32)       10640     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B3_2 (Bottleneck  (None, 28, 28, 32)       15680     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B3_3 (Bottleneck  (None, 28, 28, 32)       15680     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B4_1 (Bottleneck  (None, 14, 14, 64)       21952     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B4_2 (Bottleneck  (None, 14, 14, 64)       55936     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B4_3 (Bottleneck  (None, 14, 14, 64)       55936     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B4_4 (Bottleneck  (None, 14, 14, 64)       55936     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B5_1 (Bottleneck  (None, 14, 14, 96)       68352     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B5_2 (Bottleneck  (None, 14, 14, 96)       120768    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B5_3 (Bottleneck  (None, 14, 14, 96)       120768    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B6_1 (Bottleneck  (None, 7, 7, 160)        157888    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B6_2 (Bottleneck  (None, 7, 7, 160)        324160    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B6_3 (Bottleneck  (None, 7, 7, 160)        324160    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B7_1 (Bottleneck  (None, 7, 7, 320)        478400    \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_out (Conv2D)           (None, 7, 7, 1280)        409600    \n",
      "                                                                 \n",
      " avg_pool (AveragePooling2D)  (None, 1, 1, 1280)       0         \n",
      "                                                                 \n",
      " conv_seg (Conv2D)           (None, 1, 1, 11)          14080     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,268,096\n",
      "Trainable params: 2,236,480\n",
      "Non-trainable params: 31,616\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II03ZRu17FnE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Defining SSD<br>\n",
    "The default number of boxes per layer and resolution of each layer is different, since we are working with MNIST data and 224x224 image sizes.<p>\n",
    "To change the number of boxes per layer and layerWidths, some constraints need to be kept in mind which are mentioned in the later sections"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aYD2gfR9O8L0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class SSD(keras.Model):\n",
    "  def __init__(self, numBoxes=[4, 6, 6, 6, 4, 4], layerWidth=[28, 14, 7, 4, 2, 1], k=10+1+4):\n",
    "    super(SSD, self).__init__()\n",
    "    self.classes = k\n",
    "    self.featureMaps = 6\n",
    "    self.MobileNet = MobileNetv2(k=k)\n",
    "\n",
    "    # mark bottleneck_6_1 onwards as non trainable\n",
    "    for layer in self.MobileNet.layers[-7:]:\n",
    "      layer.trainable = False\n",
    "\n",
    "    # For bottleneck_5_3, mark layers beyond conv as non runnable\n",
    "    # layers in bottleneck_5_3: ['Bottleneck_B5_3_expand_BN', 'Bottleneck_B5_3_expand_ReLU', 'Bottleneck_B5_3_conv', 'Bottleneck_B5_3_conv_BN', \n",
    "    # 'Bottleneck_B5_3_conv_ReLU', 'contract', 'Bottleneck_B5_3_contract_BN', 'Bottleneck_B5_3_residual', 'Bottleneck_B5_3_expand']\n",
    "    for layer in self.MobileNet.layers[-8].layers[2:-1]:\n",
    "      layer.trainable = False\n",
    "\n",
    "    self.numBoxes = numBoxes\n",
    "    self.layerWidth = layerWidth\n",
    "    self.features = [None for _ in range(self.featureMaps)]\n",
    "    self.classifiers = [None for _ in range(self.featureMaps)]\n",
    "\n",
    "    self.conv1_1 = layers.Conv2D(256, 1, name='SSD_conv_1_1')\n",
    "    self.conv1_2 = layers.Conv2D(512, 3, strides=(2, 2), padding='same', name='SSD_conv_1_2')\n",
    "\n",
    "    self.conv2_1 = layers.Conv2D(128, 1, name='SSD_conv_2_1')\n",
    "    self.conv2_2 = layers.Conv2D(256, 3, strides=(2, 2), padding='same', name='SSD_conv_2_2')\n",
    "\n",
    "    self.conv3_1 = layers.Conv2D(128, 1, name='SSD_conv_3_1')\n",
    "    self.conv3_2 = layers.Conv2D(256, 3, strides=(1, 1), name='SSD_conv_3_2')\n",
    "\n",
    "    self.conv4_1 = layers.Conv2D(128, 1, name='SSD_conv_4_1')\n",
    "    self.conv4_2 = layers.Conv2D(256, 2, strides=(1, 1), name='SSD_conv_4_2')  # changed the kernel size to 2 since the output of the previous layer has width 3\n",
    "\n",
    "    self.conv = []\n",
    "    self.reshape = []\n",
    "    for i in range(self.featureMaps):\n",
    "      self.conv.append(layers.Conv2D(self.numBoxes[i]*self.classes, 3, padding='same', name='Classification_'+str(i)))\n",
    "      self.reshape.append(layers.Reshape((self.layerWidth[i]*self.layerWidth[i]*self.numBoxes[i], self.classes), name='Reshape_classification_'+str(i)))\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.MobileNet.build(input_shape)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = inputs\n",
    "    x = self.MobileNet(x)\n",
    "\n",
    "    # get the convolved images at different resolutions\n",
    "    self.features[0] = self.MobileNet.get_layer('Bottleneck_B4_1').out\n",
    "    self.features[1] = self.MobileNet.get_layer('Bottleneck_B5_3').out\n",
    "    self.features[2] = self.conv1_2(self.conv1_1(self.features[1]))\n",
    "    self.features[3] = self.conv2_2(self.conv2_1(self.features[2]))\n",
    "    self.features[4] = self.conv3_2(self.conv3_1(self.features[3]))\n",
    "    self.features[5] = self.conv4_2(self.conv4_1(self.features[4]))\n",
    "\n",
    "    for i in range(self.featureMaps):\n",
    "      # for each feature map, create predictions according to the number of boxes for that layer and the number of output channels\n",
    "      x = self.conv[i](self.features[i])\n",
    "      x = self.reshape[i](x)\n",
    "      self.classifiers[i] = x\n",
    "\n",
    "    # concatenate all the classifiers\n",
    "    x = layers.concatenate(self.classifiers, axis=-2, name='concatenate')\n",
    "    return x\n",
    "\n",
    "  def model(self):\n",
    "    x = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "    return keras.Model(inputs=x, outputs=self.call(x))"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "29A_FW-GxK4t",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "NUM_CLASSES = 10\n",
    "# the first 2 dimensions should be equal to width of the output from the bottleneck expand ReLU at the (4,1) and (5,3) respectively. the dimensions after the second one are determined by the convolutions written inside the SSD (conv1_2, conv2_2, conv3_3, conv4_2)\n",
    "numBoxes = [3,3,3,3,3,3]\n",
    "layerWidths = [28,14,7,4,2,1]\n",
    "assert len(numBoxes) == len(layerWidths) # numBoxes for each layer and each layer has a specific width\n",
    "outputChannels = NUM_CLASSES + 1 + 4 # classes + background + cx,cy,h,w\n",
    "assert outputChannels - NUM_CLASSES == 5"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FmOfPVIjCkuP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "16c53e83-aa54-44c8-bd30-43d452cce1c1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "model = SSD(numBoxes=numBoxes, layerWidth=layerWidths, k=outputChannels)\n",
    "model.model().summary()"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv (Conv2D)                  (None, 111, 111, 32  864         ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " BN (BatchNormalization)        (None, 111, 111, 32  128         ['conv[0][0]']                   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ReLU (ReLU)                    (None, 111, 111, 32  0           ['BN[0][0]']                     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Bottleneck_B1_1 (Bottleneck)   (None, 111, 111, 16  2144        ['ReLU[0][0]']                   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Bottleneck_B2_1 (Bottleneck)   (None, 56, 56, 24)   5568        ['Bottleneck_B1_1[0][0]']        \n",
      "                                                                                                  \n",
      " Bottleneck_B2_2 (Bottleneck)   (None, 56, 56, 24)   9456        ['Bottleneck_B2_1[0][0]']        \n",
      "                                                                                                  \n",
      " Bottleneck_B3_1 (Bottleneck)   (None, 28, 28, 32)   10640       ['Bottleneck_B2_2[0][0]']        \n",
      "                                                                                                  \n",
      " Bottleneck_B3_2 (Bottleneck)   (None, 28, 28, 32)   15680       ['Bottleneck_B3_1[0][0]']        \n",
      "                                                                                                  \n",
      " Bottleneck_B3_3 (Bottleneck)   (None, 28, 28, 32)   15680       ['Bottleneck_B3_2[0][0]']        \n",
      "                                                                                                  \n",
      " Bottleneck_B4_1 (Bottleneck)   (None, 14, 14, 64)   21952       ['Bottleneck_B3_3[0][0]']        \n",
      "                                                                                                  \n",
      " Bottleneck_B4_2 (Bottleneck)   (None, 14, 14, 64)   55936       ['Bottleneck_B4_1[0][0]']        \n",
      "                                                                                                  \n",
      " Bottleneck_B4_3 (Bottleneck)   (None, 14, 14, 64)   55936       ['Bottleneck_B4_2[0][0]']        \n",
      "                                                                                                  \n",
      " Bottleneck_B4_4 (Bottleneck)   (None, 14, 14, 64)   55936       ['Bottleneck_B4_3[0][0]']        \n",
      "                                                                                                  \n",
      " Bottleneck_B5_1 (Bottleneck)   (None, 14, 14, 96)   68352       ['Bottleneck_B4_4[0][0]']        \n",
      "                                                                                                  \n",
      " Bottleneck_B5_2 (Bottleneck)   (None, 14, 14, 96)   120768      ['Bottleneck_B5_1[0][0]']        \n",
      "                                                                                                  \n",
      " Bottleneck_B5_3_expand (Conv2D  (None, 14, 14, 576)  55296      ['Bottleneck_B5_2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Bottleneck_B5_3_expand_BN (Bat  (None, 14, 14, 576)  2304       ['Bottleneck_B5_3_expand[0][0]'] \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " Bottleneck_B5_3_expand_ReLU (R  (None, 14, 14, 576)  0          ['Bottleneck_B5_3_expand_BN[0][0]\n",
      " eLU)                                                            ']                               \n",
      "                                                                                                  \n",
      " SSD_conv_1_1 (Conv2D)          (None, 14, 14, 256)  147712      ['Bottleneck_B5_3_expand_ReLU[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " SSD_conv_1_2 (Conv2D)          (None, 7, 7, 512)    1180160     ['SSD_conv_1_1[0][0]']           \n",
      "                                                                                                  \n",
      " SSD_conv_2_1 (Conv2D)          (None, 7, 7, 128)    65664       ['SSD_conv_1_2[0][0]']           \n",
      "                                                                                                  \n",
      " SSD_conv_2_2 (Conv2D)          (None, 4, 4, 256)    295168      ['SSD_conv_2_1[0][0]']           \n",
      "                                                                                                  \n",
      " SSD_conv_3_1 (Conv2D)          (None, 4, 4, 128)    32896       ['SSD_conv_2_2[0][0]']           \n",
      "                                                                                                  \n",
      " Bottleneck_B4_1_expand (Conv2D  (None, 28, 28, 192)  6144       ['Bottleneck_B3_3[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " SSD_conv_3_2 (Conv2D)          (None, 2, 2, 256)    295168      ['SSD_conv_3_1[0][0]']           \n",
      "                                                                                                  \n",
      " Bottleneck_B4_1_expand_BN (Bat  (None, 28, 28, 192)  768        ['Bottleneck_B4_1_expand[0][0]'] \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " SSD_conv_4_1 (Conv2D)          (None, 2, 2, 128)    32896       ['SSD_conv_3_2[0][0]']           \n",
      "                                                                                                  \n",
      " Bottleneck_B4_1_expand_ReLU (R  (None, 28, 28, 192)  0          ['Bottleneck_B4_1_expand_BN[0][0]\n",
      " eLU)                                                            ']                               \n",
      "                                                                                                  \n",
      " SSD_conv_4_2 (Conv2D)          (None, 1, 1, 256)    131328      ['SSD_conv_4_1[0][0]']           \n",
      "                                                                                                  \n",
      " Classification_0 (Conv2D)      (None, 28, 28, 45)   77805       ['Bottleneck_B4_1_expand_ReLU[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " Classification_1 (Conv2D)      (None, 14, 14, 45)   233325      ['Bottleneck_B5_3_expand_ReLU[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " Classification_2 (Conv2D)      (None, 7, 7, 45)     207405      ['SSD_conv_1_2[0][0]']           \n",
      "                                                                                                  \n",
      " Classification_3 (Conv2D)      (None, 4, 4, 45)     103725      ['SSD_conv_2_2[0][0]']           \n",
      "                                                                                                  \n",
      " Classification_4 (Conv2D)      (None, 2, 2, 45)     103725      ['SSD_conv_3_2[0][0]']           \n",
      "                                                                                                  \n",
      " Classification_5 (Conv2D)      (None, 1, 1, 45)     103725      ['SSD_conv_4_2[0][0]']           \n",
      "                                                                                                  \n",
      " Reshape_classification_0 (Resh  (None, 2352, 15)    0           ['Classification_0[0][0]']       \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " Reshape_classification_1 (Resh  (None, 588, 15)     0           ['Classification_1[0][0]']       \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " Reshape_classification_2 (Resh  (None, 147, 15)     0           ['Classification_2[0][0]']       \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " Reshape_classification_3 (Resh  (None, 48, 15)      0           ['Classification_3[0][0]']       \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " Reshape_classification_4 (Resh  (None, 12, 15)      0           ['Classification_4[0][0]']       \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " Reshape_classification_5 (Resh  (None, 3, 15)       0           ['Classification_5[0][0]']       \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3150, 15)     0           ['Reshape_classification_0[0][0]'\n",
      "                                                                 , 'Reshape_classification_1[0][0]\n",
      "                                                                 ',                               \n",
      "                                                                  'Reshape_classification_2[0][0]'\n",
      "                                                                 , 'Reshape_classification_3[0][0]\n",
      "                                                                 ',                               \n",
      "                                                                  'Reshape_classification_4[0][0]'\n",
      "                                                                 , 'Reshape_classification_5[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,507,342\n",
      "Trainable params: 3,492,494\n",
      "Non-trainable params: 14,848\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RA7NxfQ7_G6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating boxes and IoU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yRYi7Ez7UzpH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# I have used less varying custom scales and aspect ratios here, since the dataset is already uniform\n",
    "#IMPORTANT: before changing the scales and aspect ratios, read the comment below\n",
    "\n",
    "# number of scales is equal to the number of different resolutions ie num of layer widths\n",
    "# for a given resolution, we have different aspect ratios\n",
    "# num(scales) = num(layerWidth) = num(numBoxes) and num(asp_ratios) = numBoxes[i]\n",
    "MinScale = .1 # Min and Max scale given as percentage\n",
    "MaxScale = 1.5\n",
    "scales = [ MinScale + x/len(layerWidths) * (MaxScale-MinScale) for x in range(len(layerWidths)) ]\n",
    "scales = scales[::-1] # reversing the order because the layerWidths go from high to low (lower to higher resoltuion)\n",
    "\n",
    "asp = [0.5,1.0,1.5]\n",
    "asp1 = [x**0.5 for x in asp]\n",
    "asp2 = [1/x for x in asp1]"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RwR_TIbzYCix",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "IMG_SIZE = 224"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wA_IhnyrUl4S",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "59f0db0d-5f22-4ee8-abd7-0a9d19b43e11",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# should be equal to the 1st dimension in the output layer of the SSD model\n",
    "BOXES = sum([a*a*b for a, b in zip(layerWidths, numBoxes)])\n",
    "centres = np.zeros((BOXES, 2))\n",
    "hw = np.zeros((BOXES, 2))\n",
    "boxes = np.zeros((BOXES, 4))\n",
    "print(BOXES)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3150\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9A1xGMrXVX18",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# calculating the default box centres and height, width\n",
    "idx = 0\n",
    "\n",
    "for gridSize, numBox, scale in zip(layerWidths, numBoxes, scales):\n",
    "  step_size = IMG_SIZE*1.0/gridSize\n",
    "  for i in range(gridSize):\n",
    "    for j in range(gridSize):\n",
    "      pos = idx+(i*gridSize+j)*numBox\n",
    "      # centre is the same for all aspect ratios(=numBox)\n",
    "      centres[pos: pos+numBox, :] = i*step_size+step_size/2, j*step_size+step_size/2\n",
    "      # height and width vary according to the scale and aspect ratio\n",
    "      # zip asepct ratios and then scale them by the scaling factor\n",
    "      hw[pos: pos+numBox, :] = np.multiply(gridSize*scale, np.squeeze(np.dstack([asp1, asp2]), axis=0))[:numBox, :]\n",
    "\n",
    "  idx += gridSize*gridSize*numBox"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xp9CrasJhGXI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# (x,y) co-ordinates of top left and bottom right\n",
    "# This actually is not used anywhere. centres[] and hw[] are a good enough substitute\n",
    "boxes[:,0] = centres[:,0] - hw[:,0]/2\n",
    "boxes[:,1] = centres[:,1] - hw[:,1]/2\n",
    "boxes[:,2] = centres[:,0] + hw[:,0]/2\n",
    "boxes[:,3] = centres[:,1] + hw[:,1]/2"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TJSIPHPMh3N2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# calculate IoU for a set of search boxes and default boxes\n",
    "def IoU(box1, box2):\n",
    "  box1 = box1.astype(np.float64)\n",
    "  box2 = box2.astype(np.float64)\n",
    "  # find the left and right co-ordinates of the edges. Min should be less than Max for non zero overlap\n",
    "  xmin = np.maximum(box1[:,0],box2[:,0])\n",
    "  xmax = np.minimum(box1[:,2],box2[:,2])\n",
    "  ymin = np.maximum(box1[:,1],box2[:,1])\n",
    "  ymax = np.minimum(box1[:,3],box2[:,3])\n",
    "\n",
    "  intersection = np.abs(np.maximum(xmax-xmin,0) * np.maximum(ymax-ymin,0))\n",
    "  boxArea1 = np.abs((box1[:,2] - box1[:,0]) * (box1[:,3] - box1[:,1]))\n",
    "  boxArea2 = np.abs((box2[:,2] - box2[:,0]) * (box2[:,3] - box2[:,1]))\n",
    "  unionArea = boxArea1 + boxArea2 - intersection\n",
    "  assert (unionArea > 0).all()\n",
    "  iou = intersection / unionArea\n",
    "\n",
    "  return iou"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iOcpxxIQipbA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# give the index of the box correpsonding to the IoUs > threshold (=0.5) \n",
    "def bestIoU(searchBox):\n",
    "  return np.argwhere(IoU(numpy.matlib.repmat(searchBox,BOXES,1), boxes) > 0.5)"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm2k4c5Ik_BX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h07BGB7-k9te",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "TRAINSIZE = 600\n",
    "TESTSIZE = 100"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wkZPTKgGq08N",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "48b663c1-2189-4bd9-bf3e-a2adf64c48dc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train[:TRAINSIZE , : , :]\n",
    "y_train = y_train[:TRAINSIZE]\n",
    "x_test = x_test[:TESTSIZE , : , :]\n",
    "y_test = y_test[:TESTSIZE]"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# x_train[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PEgx1Sdcq7yJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# take mnist x and y pairs and convert to input, output pairs for the MobileNetv2+SSD model\n",
    "def convert(x,y):\n",
    "  MNIST_SIZE = x.shape[-1]\n",
    "  # create a 2D array of top left corners for the mnist image to be placed\n",
    "  corner = np.random.randint(IMG_SIZE - MNIST_SIZE, size=(x.shape[0],2))\n",
    "\n",
    "  # create a blank canvas for the input with the required dimension\n",
    "  input = np.zeros((x.shape[0], IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "  # replacing a part by RGB version of MNIST\n",
    "  for i in range(x.shape[0]):\n",
    "    lx = int(corner[i,0])\n",
    "    ly = int(corner[i,1])\n",
    "    input[i,lx:lx + MNIST_SIZE, ly:ly+MNIST_SIZE,:] = np.repeat(np.expand_dims(np.array(x[i,:,:]),axis=-1),3,axis=-1)\n",
    "\n",
    "  # for each default box, there are 5 values: class number and delta cx,cy,h,w\n",
    "  output = np.zeros((y.shape[0],BOXES,1+4))\n",
    "  output[:,:,0] = NUM_CLASSES # defaulting class labels for all boxes to background initially\n",
    "  for i in range(x.shape[0]):\n",
    "    bbox = np.zeros(4)\n",
    "    bbox[:2] = corner[i]\n",
    "    bbox[2:] = corner[i] + (MNIST_SIZE,MNIST_SIZE)\n",
    "    # for all default boxes which have IoU > threshold, set the delta values and class number\n",
    "    box_idx = bestIoU(bbox).astype(np.uint16)\n",
    "    output[i,box_idx,0] = y[i]\n",
    "    output[i,box_idx,1] = (bbox[0] + bbox[2])/2.0 - centres[box_idx,0]\n",
    "    output[i,box_idx,2] = (bbox[1] + bbox[3])/2.0 - centres[box_idx,1]\n",
    "    output[i,box_idx,3] = MNIST_SIZE - hw[box_idx,0]\n",
    "    output[i,box_idx,4] = MNIST_SIZE - hw[box_idx,1]\n",
    "\n",
    "  return input, output\n"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Sk_z17wV3Bj5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "train_x, train_y = convert(x_train,y_train)\n",
    "test_x, test_y = convert(x_test,y_test)"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 224, 224, 3)\n",
      "(600, 3150, 5)\n",
      "(100, 224, 224, 3)\n",
      "(100, 3150, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "\n",
    "# train_y[99]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NAwnJnu4qE0P",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "outputId": "36105aba-89ce-4e12-ad01-2ab01a6b2785",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# checking if the inputs prepared are correct or not\n",
    "r = np.random.randint(0,train_x.shape[0])\n",
    "img = train_x[r,:,:,:].copy()\n",
    "img_y = train_y[r]\n",
    "\n",
    "im = np.array(Image.fromarray(img.astype(np.uint8)))\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(im)\n",
    "\n",
    "# find all boxes where class label is not background\n",
    "idx = np.argwhere(img_y[:,0] != NUM_CLASSES)[:,0]\n",
    "print('Number of boxes with IoU > threshold (0.5):',idx.shape[0])\n",
    "print('Green box: ground truth. Red box: default boxes with IoU < threshold (0.5)')\n",
    "\n",
    "#calculating the ground truth bounding boxes\n",
    "gt = np.zeros(4,dtype=np.uint16)\n",
    "gt[:2] = (img_y[idx[0],1:3] + centres[idx[0],:2])\n",
    "gt[2:] = (img_y[idx[0],3:] + hw[idx[0],:])\n",
    "\n",
    "# for some reason, x and y are inverted\n",
    "rect = patches.Rectangle((gt[1]-gt[3]/2,gt[0]-gt[2]/2),gt[3],gt[2],linewidth=5,edgecolor='g',facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# showing all the boxes with IoU > 0.5\n",
    "for i in idx:\n",
    "  rect = patches.Rectangle((centres[i][1]-hw[i,1]/2,centres[i][0]-hw[i,0]/2),hw[i,1],hw[i,0],linewidth=1,edgecolor='r',facecolor='none')\n",
    "  ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of boxes with IoU > threshold (0.5): 9\n",
      "Green box: ground truth. Red box: default boxes with IoU < threshold (0.5)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# test_y[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1nI7mXjS8zA9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f461fad5-a72f-4a77-92f5-30653c5209a8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "print(train_dataset.element_spec)\n",
    "print(test_dataset.element_spec)"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(224, 224, 3), dtype=tf.float64, name=None), TensorSpec(shape=(3150, 5), dtype=tf.float64, name=None))\n",
      "(TensorSpec(shape=(224, 224, 3), dtype=tf.float64, name=None), TensorSpec(shape=(3150, 5), dtype=tf.float64, name=None))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oyX8dnwQ8_1k",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "BATCH_SIZE = 10\n",
    "SHUFFLE_BUFFER_SIZE = 60\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EeBg_g29GLU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LOSS\n",
    "\n",
    "Hard negative mining hasn't been done here\n",
    "Initial idea was to assign weights to background classes, but there is some problem in that approach"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rMEpljzd9CxT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# label is not required here in the standard implementation\n",
    "def smoothL1(x, y, label):\n",
    "  diff = K.abs(x-y) #* K.switch(label == 10, label*1.0/BOXES, label)\n",
    "  result = K.switch(diff < 1, 0.5 * diff**2, diff - 0.5)\n",
    "  return K.mean(result)"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8fSUhh8O_DsK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def confidenceLoss(y, label):\n",
    "  unweighted_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(label, y)\n",
    "  # class_weights = tf.constant([[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0/BOXES]]*BOXES])\n",
    "  # weights = tf.reduce_sum(class_weights * y, axis = -1)\n",
    "  # weighted_loss = unweighted_loss * weights\n",
    "  return K.mean(unweighted_loss)"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gn0xh6OX_BvN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def Loss(gt, y):\n",
    "  # shape of y is n * BOXES * output_channels\n",
    "  # shape of gt is n * BOXES * 5 \n",
    "  loss = 0\n",
    "  # localisation loss\n",
    "  loss += smoothL1(y[:,:,-4:],gt[:,:,-4:],gt[:,:,0:1])\n",
    "  # confidence loss\n",
    "  loss += confidenceLoss(y[:,:,:-4],tf.cast(gt[:,:,0],tf.int32))\n",
    "  return loss"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A8_O3V8DB_Gk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "51c74056-5226-45a7-cf70-fa004d203946",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "base_learning_rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate),loss=Loss)"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z7pp6Fp9DDWm",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "39ca0340-fe39-4016-da7d-1ca95148010b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=25,\n",
    "                    validation_data=test_dataset,)"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 15:14:41.362945: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 15:14:41.363520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 15:14:41.363893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 15:14:41.364146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 15:14:41.982851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 15:14:41.983464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 15:14:41.983703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 15:14:41.983917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5146 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-09-23 15:14:43.054305: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-09-23 15:15:27.816415: W tensorflow/core/common_runtime/forward_type_inference.cc:332] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: type mismatch for node 'DatasetToGraphV2': expected a subtype of:\n",
      "type_id: TFT_PRODUCT\n",
      "args {\n",
      "  type_id: TFT_ENCODED\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_DOUBLE\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_DOUBLE\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_STRING\n",
      "  }\n",
      "}\n",
      "\n",
      "  got:\n",
      "type_id: TFT_PRODUCT\n",
      "args {\n",
      "  type_id: TFT_ENCODED\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_DOUBLE\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_DOUBLE\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_STRING\n",
      "  }\n",
      "}\n",
      "\n",
      "  \n",
      "\twhile updating its output type.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60 steps, validate on 10 steps\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 15:16:25.705637: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8500\n",
      "2022-09-23 15:16:26.320027: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-09-23 15:16:26.443529: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - ETA: 0s - batch: 29.5000 - size: 1.0000 - loss: 0.4220"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/.local/lib/python3.8/site-packages/keras/engine/training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 12s 93ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.4220 - val_loss: 0.3254\n",
      "Epoch 2/25\n",
      "60/60 [==============================] - 3s 53ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0913 - val_loss: 0.0540\n",
      "Epoch 3/25\n",
      "60/60 [==============================] - 3s 53ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0809 - val_loss: 0.2373\n",
      "Epoch 4/25\n",
      "60/60 [==============================] - 3s 53ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0421 - val_loss: 0.1290\n",
      "Epoch 5/25\n",
      "60/60 [==============================] - 3s 53ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0342 - val_loss: 0.0396\n",
      "Epoch 6/25\n",
      "60/60 [==============================] - 3s 56ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0363 - val_loss: 0.0267\n",
      "Epoch 7/25\n",
      "60/60 [==============================] - 3s 53ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0284 - val_loss: 0.0361\n",
      "Epoch 8/25\n",
      "60/60 [==============================] - 3s 54ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0302 - val_loss: 0.0232\n",
      "Epoch 9/25\n",
      "60/60 [==============================] - 3s 53ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0549 - val_loss: 0.0176\n",
      "Epoch 10/25\n",
      "60/60 [==============================] - 3s 55ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0152 - val_loss: 0.0181\n",
      "Epoch 11/25\n",
      "60/60 [==============================] - 3s 56ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0188 - val_loss: 0.0155\n",
      "Epoch 12/25\n",
      "60/60 [==============================] - 3s 55ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0243 - val_loss: 0.0146\n",
      "Epoch 13/25\n",
      "60/60 [==============================] - 3s 57ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0175 - val_loss: 0.0137\n",
      "Epoch 14/25\n",
      "60/60 [==============================] - 4s 59ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0186 - val_loss: 0.0752\n",
      "Epoch 15/25\n",
      "60/60 [==============================] - 3s 54ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0155 - val_loss: 0.0127\n",
      "Epoch 16/25\n",
      "60/60 [==============================] - 3s 54ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0108 - val_loss: 0.0126\n",
      "Epoch 17/25\n",
      "60/60 [==============================] - 3s 56ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0146 - val_loss: 0.0226\n",
      "Epoch 18/25\n",
      "60/60 [==============================] - 4s 59ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0114 - val_loss: 0.0173\n",
      "Epoch 19/25\n",
      "60/60 [==============================] - 3s 53ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 20/25\n",
      "60/60 [==============================] - 3s 54ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 21/25\n",
      "60/60 [==============================] - 3s 53ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0104 - val_loss: 0.0377\n",
      "Epoch 22/25\n",
      "60/60 [==============================] - 3s 54ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0104 - val_loss: 0.0452\n",
      "Epoch 23/25\n",
      "60/60 [==============================] - 3s 55ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0077 - val_loss: 0.0202\n",
      "Epoch 24/25\n",
      "60/60 [==============================] - 3s 53ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0084 - val_loss: 0.0095\n",
      "Epoch 25/25\n",
      "60/60 [==============================] - 3s 54ms/step - batch: 29.5000 - size: 1.0000 - loss: 0.0088 - val_loss: 0.0151\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g_n2VfMsg1NT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "e20fc8d4-968c-4272-9c62-57eeefcee1b3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "model.evaluate(test_x,test_y)"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "0.015091702789068222"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_save_path = \"temp.h5\"\n",
    "# model.save(model_save_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oNuY-45SngR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VTfYjsyJTEtv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# create some sample data\n",
    "X, Y = convert(x_test, y_test)"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QPazH1zFTnE4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "535d81f5-7835-47ce-98a0-6d78bfe13c78",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# get prediction for one sample\n",
    "y_pred = model.predict(X)\n",
    "y_pred.shape"
   ],
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/.local/lib/python3.8/site-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "data": {
      "text/plain": "(100, 3150, 15)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jrD03dgjcZMO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "OBJperCLASS = 10 # get top 10 results for each class\n",
    "# get the confidence scores (with class values) and delta for the boxes. For each class, the top 10 values are used\n",
    "def infer(Y):\n",
    "  # classes are actually the index into the default boxes\n",
    "  classes = np.zeros((OBJperCLASS,outputChannels-4),dtype=np.uint16)\n",
    "  conf = np.zeros((OBJperCLASS,outputChannels-4))\n",
    "  delta = np.zeros((OBJperCLASS,outputChannels-4,4))\n",
    "  class_predictions = softmax(Y[:,:outputChannels-4],axis=1)\n",
    "  for i in range(outputChannels-4):\n",
    "    classes[:,i] = bottleneck.argpartition(class_predictions[:,i],BOXES-1-10,axis=-1)[-OBJperCLASS:]\n",
    "    conf[:,i] = class_predictions[classes[:,i],i]\n",
    "    delta[:,i] = Y[classes[:,i],outputChannels-4:]\n",
    "  return conf,classes, delta\n",
    "\n",
    "# generate bounding boxes from the inferred outputs\n",
    "def Bbox(confidence,box_idx,delta):\n",
    "  #delta contains delta(cx,cy,h,w)\n",
    "  bbox_centre = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
    "  bbox_hw = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
    "  for i in range(OBJperCLASS):\n",
    "    bbox_centre[i,:,0] = centres[box_idx[i]][:,0]+delta[i,:,0]\n",
    "    bbox_centre[i,:,1] = centres[box_idx[i]][:,1]+delta[i,:,1]\n",
    "    bbox_hw[i,:,0] = hw[box_idx[i]][:,0] + delta[i,:,2]\n",
    "    bbox_hw[i,:,1] = hw[box_idx[i]][:,1]+delta[i,:,3]\n",
    "  return bbox_centre,bbox_hw"
   ],
   "execution_count": 84,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LY7SOlpafX51",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "outputId": "eef5e296-27f6-46f1-bfeb-422573f1d1e1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "r = np.random.randint(TESTSIZE)\n",
    "print(f\"test_index: {r}\")\n",
    "# r = 33\n",
    "\n",
    "# top 10 predictions for each class\n",
    "confidence, box_idx, delta = infer(y_pred[r])\n",
    "bbox_centre, bbox_hw = Bbox(confidence, box_idx, delta)\n",
    "# print(f\"confidence: {confidence}, box_idx: {box_idx}, delta: {delta}{chr(10)}len: {len(confidence)}\")\n",
    "# print(f\"bbox_centre: {bbox_centre}, bbox_hw: {bbox_hw}\")\n",
    "\n",
    "im = np.array(Image.fromarray(X[r].astype(np.uint8)))\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(im)\n",
    "\n",
    "for i in range(outputChannels-4):\n",
    "  # skipping backgrounds\n",
    "  if i == NUM_CLASSES:\n",
    "    continue\n",
    "  color = 'r'\n",
    "  # if a class is mentioned in the ground truth, color the boxes green\n",
    "  if i in Y[r, :, 0]:\n",
    "    color = 'g'\n",
    "    print(f\"answer: {i}\")\n",
    "\n",
    "  # skip all the classes which have low confidence values\n",
    "  if (confidence[:, i] > 0.5).any() or i in Y[r, :, 0]:\n",
    "    for k in range(OBJperCLASS):\n",
    "      print(f\"{i}: Confidence-{confidence[k,i]} Centre-{bbox_centre[k,i]} Height,Width-{bbox_hw[k,i]}\")\n",
    "      # draw bounding box only if confidence scores are high\n",
    "      if confidence[k, i] < 0.5:\n",
    "        continue\n",
    "      x = bbox_centre[k, i, 0]-bbox_hw[k, i, 0]/2\n",
    "      y = bbox_centre[k, i, 1]-bbox_hw[k, i, 1]/2\n",
    "      rect = patches.Rectangle((y, x), bbox_hw[k, i, 1], bbox_hw[k, i, 0], linewidth=1, edgecolor=color, facecolor='none')\n",
    "      ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Confidence-0.027854114770889282 Centre-[76.55178952 14.30103564] Height,Width-[25.80721527 43.19864976]\n",
      "0: Confidence-0.0019235553918406367 Centre-[59.97372102  4.03878973] Height,Width-[43.39520587 28.86050693]\n",
      "0: Confidence-0.003032579319551587 Centre-[52.07945193  4.0911904 ] Height,Width-[43.3769962  28.90248902]\n",
      "0: Confidence-0.029955800622701645 Centre-[76.74382764 26.28398001] Height,Width-[26.82780987 30.8485726 ]\n",
      "0: Confidence-0.05562914162874222 Centre-[77.56589448 21.05212104] Height,Width-[35.06951603 28.02977653]\n",
      "0: Confidence-0.1444317102432251 Centre-[80.34461212 27.29429078] Height,Width-[33.91752323 27.85670664]\n",
      "0: Confidence-0.2651345133781433 Centre-[76.74432516 23.03448677] Height,Width-[26.70224982 32.46934973]\n",
      "0: Confidence-0.5247583985328674 Centre-[78.13430142 25.3996141 ] Height,Width-[26.46089694 26.56713053]\n",
      "0: Confidence-0.584536075592041 Centre-[77.85530877 22.33756971] Height,Width-[27.17303893 27.09712454]\n",
      "0: Confidence-0.6027975678443909 Centre-[77.45097113 26.39724779] Height,Width-[33.51749405 28.03271212]\n",
      "2: Confidence-0.03253408521413803 Centre-[81.05597854 26.68518877] Height,Width-[31.78993985 31.77465555]\n",
      "2: Confidence-0.04961384832859039 Centre-[76.74382764 26.28398001] Height,Width-[26.82780987 30.8485726 ]\n",
      "2: Confidence-0.526229739189148 Centre-[80.34461212 27.29429078] Height,Width-[33.91752323 27.85670664]\n",
      "2: Confidence-0.7538367509841919 Centre-[81.04593778 21.26312912] Height,Width-[34.91339001 28.46434279]\n",
      "2: Confidence-0.0645136684179306 Centre-[77.85530877 22.33756971] Height,Width-[27.17303893 27.09712454]\n",
      "2: Confidence-0.1600785255432129 Centre-[77.45097113 26.39724779] Height,Width-[33.51749405 28.03271212]\n",
      "2: Confidence-0.13979023694992065 Centre-[76.74432516 23.03448677] Height,Width-[26.70224982 32.46934973]\n",
      "2: Confidence-0.1298818588256836 Centre-[78.13430142 25.3996141 ] Height,Width-[26.46089694 26.56713053]\n",
      "2: Confidence-0.6440007090568542 Centre-[77.56589448 21.05212104] Height,Width-[35.06951603 28.02977653]\n",
      "2: Confidence-0.48013001680374146 Centre-[81.10147524 21.06160223] Height,Width-[31.77470681 31.79474804]\n",
      "answer: 4\n",
      "4: Confidence-0.02629760280251503 Centre-[80.34461212 27.29429078] Height,Width-[33.91752323 27.85670664]\n",
      "4: Confidence-0.010223079472780228 Centre-[52.07945193  4.0911904 ] Height,Width-[43.3769962  28.90248902]\n",
      "4: Confidence-0.010733120143413544 Centre-[77.45097113 26.39724779] Height,Width-[33.51749405 28.03271212]\n",
      "4: Confidence-0.05467335879802704 Centre-[78.13430142 25.3996141 ] Height,Width-[26.46089694 26.56713053]\n",
      "4: Confidence-0.024544458836317062 Centre-[76.74382764 26.28398001] Height,Width-[26.82780987 30.8485726 ]\n",
      "4: Confidence-0.01260747853666544 Centre-[77.56589448 21.05212104] Height,Width-[35.06951603 28.02977653]\n",
      "4: Confidence-0.11303787678480148 Centre-[77.85530877 22.33756971] Height,Width-[27.17303893 27.09712454]\n",
      "4: Confidence-0.05963416397571564 Centre-[76.74432516 23.03448677] Height,Width-[26.70224982 32.46934973]\n",
      "4: Confidence-0.025993363931775093 Centre-[76.55178952 14.30103564] Height,Width-[25.80721527 43.19864976]\n",
      "4: Confidence-0.026550404727458954 Centre-[81.05597854 26.68518877] Height,Width-[31.78993985 31.77465555]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z32HPrzihVqN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}